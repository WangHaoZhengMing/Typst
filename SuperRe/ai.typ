// --- 文档基础设置 ---
#set document(title: "SRCNN 深度学习图像超分辨率详解", author: "学习中的你")
#set page(
  paper: "a4",
  margin: (left: 2.5cm, right: 2.5cm, top: 3cm, bottom: 3cm),
)
#set text(font: "pingfang sc", size: 11pt, lang: "zh") // 可以替换成支持中文的字体，如 "SimSun" 等

// --- 标题样式 ---
#set heading(numbering: "1.") // 一级标题带编号
#show heading.where(level: 1): it => {
  v(1.5em, weak: true) // 标题上间距
  strong(it)
  v(0.8em, weak: true) // 标题下间距
}
#show heading.where(level: 2): it => {
  v(1.2em, weak: true)
  text(weight: "semibold", it) // 二级标题加粗
  v(0.6em, weak: true)
}
#show heading.where(level: 3): it => {
  v(1em, weak: true)
  text(style: "italic", it) // 三级标题斜体
  v(0.5em, weak: true)
}



= SRCNN 深度学习图像超分辨率详解

== 引言

单图像超分辨率 (Single Image Super-Resolution, SISR) 是计算机视觉领域的一个经典问题，其目标是从一张低分辨率 (Low-Resolution, LR) 图像恢复出对应的高分辨率 (High-Resolution, HR) 图像。由于 LR 图像丢失了大量细节信息，这是一个病态问题 (ill-posed problem)。

传统的 SR 方法通常依赖于复杂的先验知识或手工设计的特征。而 #emph[Chao Dong] 等人在 2014-2015 年提出的 #emph[SRCNN (Super-Resolution Convolutional Neural Network)] 开创性地将深度学习，特别是卷积神经网络 (Convolutional Neural Network, CNN)，成功应用于端到端 (End-to-End) 的图像超分辨率任务。SRCNN 以其简洁的结构和在当时领先的性能，证明了深度学习在解决底层视觉问题上的巨大潜力。

本文档旨在详细解析 SRCNN 的工作原理、网络结构、关键步骤以及参数学习过程，并解答学习过程中可能遇到的核心问题。

== 背景：学习过程中的核心疑问

在学习 SRCNN 的过程中，我们主要关注以下几个核心问题：

+ SRCNN 网络结构中的三个主要步骤（提取特征、非线性映射、重建图像）具体是如何工作的？它们在网络中对应哪些操作？
+ 论文中提到的"探测器"、"转换器"、"合成器"（即网络中的卷积核参数）是如何得到的？它们是人为设计的吗？
+ 如果不是人为设计的，那么网络是如何学习到这些有效的参数的？具体的学习过程是怎样的？
+ 如何用代码（例如 Python 和深度学习框架）来定义 SRCNN 的网络结构？

接下来，我们将围绕这些问题，深入剖析 SRCNN 的细节。

== SRCNN 架构详解

SRCNN 的核心思想是直接学习一个从插值后的低分辨率图像 `Y` 到高分辨率图像 `X` 的端到端映射函数 `F`。这个映射函数 `F` 被实现为一个三层的卷积神经网络。

// 概念图描述 (可以考虑在 Typst 中插入实际图片，这里用文字描述)
// (概念图: 输入 Y -> [卷积+ReLU] -> [卷积+ReLU] -> [卷积] -> 输出 F(Y) )

=== 步骤一：图像块提取与特征表示 (Patch Extraction and Representation)

- #emph[目标：] 从输入的（经过 Bicubic 插值放大但仍模糊的）图像 `Y` 中，提取出基础的局部模式，如边缘、角点、纹理等，并将这些模式表示为一组特征。
- #emph[对应网络层：] #emph[第一层卷积 + ReLU 激活函数]。
- #emph[工作原理：]
  1. #emph[卷积操作 (Convolution):] 使用 `n1` 个卷积核（Filters / Kernels，记为 `W1`），每个核的大小为 `c × f1 × f1`（其中 `c` 是输入图像的通道数，对于灰度图 `c=1`，彩色图 `c=3`；`f1` 是卷积核的空间尺寸，论文中常用 `f1=9`）。这些卷积核就像是各种不同类型的“基础模式探测器”，它们在输入图像 `Y` 上滑动进行扫描。
  2. #emph[特征图生成 (Feature Maps):] 每个卷积核与图像卷积后，会产生一张“响应图”，图上某个位置的值表示该卷积核所代表的模式在该位置出现的强度。`n1` 个卷积核就生成了 `n1` 张特征图。这 `n1` 张特征图构成了对输入图像 `Y` 的初步特征描述。
  3. #emph[特征表示:] 对于输入图像上的任一局部区域（或称图像块 Patch），经过第一层卷积后，可以用 `n1` 个特征图在该位置的响应值（一个 `n1` 维的向量）来表示这个区域的特征。
  4. #emph[ReLU 激活函数 (Rectified Linear Unit):] 对卷积操作的输出应用 ReLU 函数 (`max(0, x)`)。这引入了非线性，使得网络能够学习更复杂的特征，并有助于提高训练效率。它相当于过滤掉了不显著的负响应。
- #emph[输出：] `n1` 张包含了基础模式信息的特征图。

=== 步骤二：非线性映射 (Non-linear Mapping)

*   *目标：* 这是 SRCNN 的核心。它负责将第一步提取到的低分辨率特征，通过一个复杂的、非线性的转换规则，映射（或“翻译”）成高分辨率图像应该具有的特征。
* *对应网络层：* *第二层卷积 + ReLU 激活函数*。
*   *工作原理：*
    1.  *卷积操作:* 使用 `n2` 个卷积核（记为 `W2`），每个核的大小为 `n1 × f2 × f2`。注意，这里的输入通道数是 `n1`（第一层的输出特征图数量）。
    2.  *特征转换:* 这里的卷积操作不再直接作用于原始像素，而是作用于第一层输出的 `n1` 张特征图。每个 `W2` 卷积核会同时考虑输入特征图上一个局部区域内（`f2 × f2` 大小）所有 `n1` 个通道的特征值，然后计算出一个新的输出值。
    3.  *映射关系:* 这一层学习的是从“低分辨率特征空间”到“高分辨率特征空间”的映射。当 `f2=1`（论文初始设定）时，它在每个空间位置上独立地将 `n1` 维的低分辨率特征向量映射到一个 `n2` 维的高分辨率特征向量。当 `f2 > 1`（如 `f2=3` 或 `f2=5`），则映射时会考虑邻近位置的特征信息。
    4.  *非线性引入:* 同样应用 ReLU 激活函数，使得映射关系可以是非线性的，能够捕捉更复杂的低分辨率到高分辨率特征的对应关系。
* *输出：* `n2` 张包含了预测的高分辨率图像特征的特征图。

=== 步骤三：图像重建 (Reconstruction)

*   *目标：* 将第二步生成的、代表高分辨率特征的 `n2` 张特征图，融合成最终输出的高分辨率图像。
* *对应网络层：* *第三层卷积*。
*   *工作原理：*
    1.  *卷积操作:* 使用 `c` 个卷积核（记为 `W3`），每个核的大小为 `n2 × f3 × f3`。这里的输入通道数是 `n2`。输出通道数 `c` 与最终目标图像的通道数一致（灰度图 `c=1`，彩色图 `c=3`）。
    2.  *特征融合与聚合:* 这里的卷积操作可以看作是对前面预测的高分辨率特征进行加权平均或智能融合。每个 `W3` 卷积核观察输入特征图上一个 `f3 × f3` 区域内所有 `n2` 个通道的特征信息，学习如何将这些信息组合起来，生成最终图像在该位置的像素值。这类似于传统方法中对重叠的高清块进行平均，但这里的融合方式是*通过学习得到的*。
    3.  *线性输出:* 这一层通常*不使用* ReLU 激活函数，因为输出的是最终的像素值，需要能够表示整个像素值范围，是一个线性的组合过程。
* *输出：* 最终预测的高分辨率图像 `F(Y)`。

== 网络参数如何获得：训练过程

SRCNN 网络中的所有卷积核权重 (`W1`, `W2`, `W3`) 和偏置 (`B1`, `B2`, `B3`)——也就是我们之前比喻的“探测器”、“转换器”、“合成器”——*并不是人为设计的，而是通过在大量数据上进行* *训练* *自动学习得到的*。

训练过程遵循监督学习的范式，主要包含以下要素：

1. *训练数据 (Training Data):*
  *   需要大量的成对样本：*低分辨率输入 `Y`* 和对应的 *真实高分辨率目标 `X`*。
    * 通常通过以下方式准备：
  *   收集大量高质量的 HR 图像。
        * 将 HR 图像进行降采样（如下采样因子为 `s`）得到 LR 图像。
  *   将 LR 图像通过简单的 Bicubic 插值放大 `s` 倍，得到与 HR 图像尺寸一致的模糊输入 `Y`。
    * 训练时，通常将 `Y` 和 `X` 切割成大量的小图像块 (Patches) 以增加数据量和提高效率。

2. *网络初始化 (Network Initialization):*
  *   搭建好 SRCNN 的三层网络结构。
    * 网络内部的所有权重 `W` 和偏置 `B` 在训练开始前进行*随机初始化*（例如，从均值为 0、标准差很小的高斯分布中采样）。此时网络是“无知”的。

3. *损失函数 (Loss Function):*
  *   用于衡量网络预测输出 `F(Y)` 与真实目标 `X` 之间的差距。
    * SRCNN 使用*均方误差 (Mean Squared Error, MSE)*：
  `L(Theta) = (1/N) * sum(|| F(Y_i; Theta) - X_i ||^2)`
  其中 `Theta` 代表网络的所有参数 (`W` 和 `B`)，`N` 是训练样本数量，`||...||^2` 表示 L2 范数的平方（即逐像素差的平方和）。
  \* 训练的目标是找到一组参数 `Theta`，使得这个损失函数 `L` 的值最小。

4. *优化算法 (Optimization Algorithm):*
  *   用于根据损失函数调整网络参数 `Theta` 以最小化损失。
    * SRCNN 使用*随机梯度下降 (Stochastic Gradient Descent, SGD)* 及其变种。基本流程如下：
  *   *前向传播 (Forward Pass):* 将一小批 (mini-batch) 训练输入 `Y` 送入网络，计算得到预测输出 `F(Y)`。
        * *计算损失 (Compute Loss):* 使用 MSE 计算 `F(Y)` 和对应 `X` 之间的损失值。
  *   *反向传播 (Backward Pass / Backpropagation):* 利用微积分链式法则，计算损失函数关于网络中*每一个*参数（`W` 和 `B`）的*梯度* (gradient)。梯度指示了参数调整的方向，以最快地减小损失。
        * *参数更新 (Parameter Update):* 根据计算出的梯度，按照一定的*学习率* (learning rate) `eta` 更新所有参数：
  `Theta_new = Theta_old - eta * gradient`

5. *迭代训练 (Iteration):*
  *   重复步骤 4 无数次（遍历整个训练数据集称为一个 Epoch），每次使用不同的小批量数据。
    * 随着迭代的进行，网络的参数 `Theta` 会逐渐收敛到一组使损失函数接近最小值的状态。此时，网络就学会了如何将模糊的 `Y` 有效地转换为清晰的 `X`。

== 代码示例：使用 PyTorch 定义 SRCNN 结构

下面是一个使用 Python 和 PyTorch 框架定义 SRCNN 网络结构的示例代码片段。这只是结构定义，实际使用还需要包含训练数据的加载、训练循环、优化器设置等。

```python
import torch
import torch.nn as nn

class SRCNN(nn.Module):
    def __init__(self, num_channels=1, n1=64, n2=32, f1=9, f2=1, f3=5):
        """
        初始化 SRCNN 网络层.

        Args:
            num_channels (int): 输入和输出图像的通道数 (灰度图为 1, 彩色图为 3).
            n1 (int): 第一个卷积层输出的特征图数量.
            n2 (int): 第二个卷积层输出的特征图数量.
            f1 (int): 第一个卷积核的空间尺寸.
            f2 (int): 第二个卷积核的空间尺寸.
            f3 (int): 第三个卷积核的空间尺寸.
        """
        super(SRCNN, self).__init__()

        # 步骤一: 图像块提取与特征表示
        # 输入通道: num_channels, 输出通道: n1, 卷积核大小: f1 x f1
        self.conv1 = nn.Conv2d(num_channels, n1, kernel_size=f1, padding=f1 // 2)
        # 使用 padding 来保持特征图尺寸与输入一致 (论文原始实现可能不同)

        # 步骤二: 非线性映射
        # 输入通道: n1, 输出通道: n2, 卷积核大小: f2 x f2
        self.conv2 = nn.Conv2d(n1, n2, kernel_size=f2, padding=f2 // 2)

        # 步骤三: 图像重建
        # 输入通道: n2, 输出通道: num_channels, 卷积核大小: f3 x f3
        self.conv3 = nn.Conv2d(n2, num_channels, kernel_size=f3, padding=f3 // 2)

        # 激活函数
        self.relu = nn.ReLU(inplace=True) # inplace=True 节省内存

    def forward(self, x):
        """
        定义数据在网络中的前向传播路径.

        Args:
            x (torch.Tensor): 输入的低分辨率图像 (经过插值放大).
        """
        x = self.relu(self.conv1(x)) # 第一层卷积 + ReLU
        x = self.relu(self.conv2(x)) # 第二层卷积 + ReLU
        x = self.conv3(x)             # 第三层卷积 (无 ReLU)
        return x
```

*代码说明:*
- 该代码定义了一个名为 `SRCNN` 的类，继承自 PyTorch 的 `nn.Module`。
- `__init__` 方法中初始化了三个 `nn.Conv2d` 卷积层和 ReLU 激活函数。注意这里使用了 `padding` 参数来使得卷积后的特征图尺寸与输入保持一致，这在实践中很常见，但需要注意论文原文训练时可能没有使用 padding，导致输出尺寸略小于输入。
- `forward` 方法定义了数据从输入到输出流经各层的顺序。

